{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPLtY0uI5SufxD4s3ycLM4a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jfzhang726/Finetune-llama3/blob/main/finetune_llamma3_gsm8k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lxVgwxsRKRpa",
        "outputId": "23c803be-27a6-45d1-daf2-999b10320329"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.1+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kr6PqYgR5Kfj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "\n",
        "# Colab has torch 2.2.1 which will break the package, so install package seperatetly\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "if major_version >= 8:\n",
        "  # new gpu Ampere, Hooper GPU (RTX 30xx, RTX 40xx,, A100, H100, L40)\n",
        "  !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "  # old GPU V100, Tesla, T4, RTX 20xx\n",
        "  !pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEcJBuo6_mwI",
        "outputId": "cc6f82de-96d6-48c4-d6fb-8ead8a458e48"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny0VUGBFBUtv",
        "outputId": "486f9595-ee98-4d4e-deae-026d8992c461"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaSdpaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "from datasets import load_dataset\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "alpaca_prompt_gsm8k = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "\n",
        "### Instruction:\n",
        "You are a helpful math teacher.\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\n",
        "{output}\n",
        "\"\"\"\n",
        "\n",
        "def formatting_prompts_func_train(examples):\n",
        "\n",
        "  questions = examples[\"question\"]\n",
        "  answers = examples[\"answer\"]\n",
        "  texts = []\n",
        "  for question, answer in zip(questions, answers):\n",
        "    # must add EOS_TOKEN, otherwise the generation never stops\n",
        "    text = alpaca_prompt_gsm8k.format(input=f\"Question: {question}\", output=f\"Answer: {answer}\") + EOS_TOKEN\n",
        "    texts.append(text)\n",
        "  return {'text': texts}\n",
        "\n",
        "\n",
        "def formatting_prompts_func_test(examples):\n",
        "\n",
        "  questions = examples[\"question\"]\n",
        "  texts = []\n",
        "  for question in questions:\n",
        "    # must add EOS_TOKEN, otherwise the generation never stops\n",
        "    text = alpaca_prompt_gsm8k.format(input=f\"Question: {question}\", output=\"\")\n",
        "    texts.append(text)\n",
        "  return {'text': texts}\n",
        "dataset_train = load_dataset('gsm8k', 'socratic', split='train')\n",
        "dataset_train = dataset_train.map(formatting_prompts_func_train, batched=True)\n",
        "\n",
        "dataset_test = load_dataset('gsm8k', 'socratic', split='test')\n",
        "dataset_test = dataset_test.map(formatting_prompts_func_test, batched=True)"
      ],
      "metadata": {
        "id": "GRsAR1QcqJ6a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test before finetune\n",
        "\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer([\n",
        "    dataset_test['text'][0]\n",
        "], return_tensors='pt').to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsMUJAIQByWd",
        "outputId": "97602397-c8f3-445d-e6c1-b5f29d8d7127"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "\n",
            "### Instruction:\n",
            "You are a helpful math teacher.\n",
            "\n",
            "### Input:\n",
            "Question: Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
            "\n",
            "### Response:\n",
            "\n",
            "<|end_of_text|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer([\n",
        "    dataset_test['text'][1]\n",
        "], return_tensors='pt').to(\"cuda\")\n",
        "\n",
        "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pULRTVL-P-7a",
        "outputId": "a5cf7fd0-2dcf-42dc-9295-bce2fe04792d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "\n",
            "### Instruction:\n",
            "You are a helpful math teacher.\n",
            "\n",
            "### Input:\n",
            "Question: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\n",
            "\n",
            "### Response:\n",
            "\n",
            "<|end_of_text|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters for training\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16, # 8, 16, 32, 64, 128\n",
        "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing='unsloth',\n",
        "    random_state=42,\n",
        "    use_rslora=False,\n",
        "  loftq_config=None\n",
        "\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP0-0GR4RdeM",
        "outputId": "7a34af16-3a1e-4f13-df2e-81243591c63d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8iQsPu8Sonw",
        "outputId": "b9368422-4714-4479-9a4e-814cc6f29a8f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaSdpaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm()\n",
              "            (post_attention_layernorm): LlamaRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset_train,\n",
        "    dataset_text_field='text',\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False, # can speed up training of short sequences for 5 times\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=3,\n",
        "        warmup_steps=5,\n",
        "        num_train_epochs=1, # it will take 2+ hours for gsm8k\n",
        "        # max_steps=60, # finetune steps, which will override num_train_epochs\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim='adamw_8bit',\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type='linear',\n",
        "        seed=42,\n",
        "        output_dir='outputs'\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "mT4h1y8rSzVw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pi5Goa79T_me",
        "outputId": "92e633fc-712e-4051-af38-38961e8bbb8d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 7,473 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 3\n",
            "\\        /    Total batch size = 6 | Total steps = 1,245\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1245' max='1245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1245/1245 2:08:46, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.620200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.002400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.694500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.772200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.607700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.508200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.400100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.262700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.013500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.938700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.952400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.763100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.693100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.881000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.826900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.790800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.821900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.747600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.749000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.718200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.719100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.868400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.743500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.713100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.781500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.720800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.772200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.682800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.810200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.665400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.855900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.769700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.773500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.686400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.719500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.636000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.813500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.755500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.751500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.844800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.736600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.726400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.656300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.717400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.808500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.641100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.584900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.698700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.585100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.649500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.669300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.835800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.751300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.671500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.698100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.712800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.644700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.717800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.830200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.766100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.663100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.695200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.669000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.765000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.510600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.615800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.759100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.771300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.715700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.714400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.633500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.642800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.665000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.698600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.738200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.658400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.671700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.742000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.639300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.773300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.636100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.611100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.650400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.726600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.802900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.824800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.758400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.782300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.632800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.682300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.648200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.606000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.590700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.643100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.608700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.750300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.688600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.603600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.677500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.666900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.760300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.698500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.632200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.633300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.637500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.759700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.689200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.797100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.829000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.560100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.652700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.580400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.743300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.740200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.700200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.719900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.725000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.657300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.692800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.727900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.610900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.670500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.691500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.785300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.627300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.616400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.678600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.712100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.709100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.589400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.731400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.705300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.644000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>0.676600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.671500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.695700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.662000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.716800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.665500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.623000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.708700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.743800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.790500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.630800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.616600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.795100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.617300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.582500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.594100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.691200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.779100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.594700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>0.683800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.777500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.663900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.662200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>0.820300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>0.729100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.684900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.729200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>0.574200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>0.694100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>0.681300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>0.608000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.633900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>0.651800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>0.713700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.752500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>0.607500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.516300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.598900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.500200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>0.611200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>0.732400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.680100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>0.683700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>0.619400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.659400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>0.512300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.635800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>0.606100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.705700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>0.698700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>0.686500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>0.739600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.692100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.580500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>0.661300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>0.676600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.793900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>0.708000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.625900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>0.716700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>0.621100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.680700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.668900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>0.616800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>0.671200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>0.683300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.760200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>201</td>\n",
              "      <td>0.601500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>202</td>\n",
              "      <td>0.649900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>203</td>\n",
              "      <td>0.746800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>0.643600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>0.633900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>206</td>\n",
              "      <td>0.593000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>207</td>\n",
              "      <td>0.647600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>0.627100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>209</td>\n",
              "      <td>0.590000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.716100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>211</td>\n",
              "      <td>0.666600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>0.645700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>213</td>\n",
              "      <td>0.778100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>214</td>\n",
              "      <td>0.649900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>0.730000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>0.758800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>217</td>\n",
              "      <td>0.610000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>218</td>\n",
              "      <td>0.683100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>219</td>\n",
              "      <td>0.685400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.703700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>221</td>\n",
              "      <td>0.695000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>222</td>\n",
              "      <td>0.716400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>223</td>\n",
              "      <td>0.800700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.556800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.713100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>226</td>\n",
              "      <td>0.680700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>227</td>\n",
              "      <td>0.658300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.773400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>229</td>\n",
              "      <td>0.722400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.577700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>0.734000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>0.521400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>233</td>\n",
              "      <td>0.707100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>234</td>\n",
              "      <td>0.735600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>0.676800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>0.603500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>237</td>\n",
              "      <td>0.617000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>238</td>\n",
              "      <td>0.669000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>239</td>\n",
              "      <td>0.658200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.632200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>241</td>\n",
              "      <td>0.733400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>242</td>\n",
              "      <td>0.670800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>243</td>\n",
              "      <td>0.557100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>0.714600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>0.810800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>246</td>\n",
              "      <td>0.674000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>247</td>\n",
              "      <td>0.699000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>0.603100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>249</td>\n",
              "      <td>0.683100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.696600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>251</td>\n",
              "      <td>0.582200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.741200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>253</td>\n",
              "      <td>0.691800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>254</td>\n",
              "      <td>0.612200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>0.555000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>256</td>\n",
              "      <td>0.681400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>257</td>\n",
              "      <td>0.647200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>258</td>\n",
              "      <td>0.497200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>259</td>\n",
              "      <td>0.691200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.687700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>261</td>\n",
              "      <td>0.550100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>262</td>\n",
              "      <td>0.677800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>263</td>\n",
              "      <td>0.652400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>264</td>\n",
              "      <td>0.559000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>0.673600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>266</td>\n",
              "      <td>0.709000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>267</td>\n",
              "      <td>0.595900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>268</td>\n",
              "      <td>0.597000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>269</td>\n",
              "      <td>0.663700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.653700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>271</td>\n",
              "      <td>0.669200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>272</td>\n",
              "      <td>0.627200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>273</td>\n",
              "      <td>0.712000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>274</td>\n",
              "      <td>0.770200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.722900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>276</td>\n",
              "      <td>0.628600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>277</td>\n",
              "      <td>0.703700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>278</td>\n",
              "      <td>0.753000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>279</td>\n",
              "      <td>0.837100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.602700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>281</td>\n",
              "      <td>0.639800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>282</td>\n",
              "      <td>0.765400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>283</td>\n",
              "      <td>0.681600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>284</td>\n",
              "      <td>0.620900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.658600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>286</td>\n",
              "      <td>0.616600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>287</td>\n",
              "      <td>0.599600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>288</td>\n",
              "      <td>0.644200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>289</td>\n",
              "      <td>0.641000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.668400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>291</td>\n",
              "      <td>0.613300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>292</td>\n",
              "      <td>0.614100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>293</td>\n",
              "      <td>0.796600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>294</td>\n",
              "      <td>0.656400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>0.576200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>296</td>\n",
              "      <td>0.640100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>297</td>\n",
              "      <td>0.638200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>298</td>\n",
              "      <td>0.700300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>299</td>\n",
              "      <td>0.621100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.661100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>301</td>\n",
              "      <td>0.629800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>302</td>\n",
              "      <td>0.741500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>303</td>\n",
              "      <td>0.657700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>304</td>\n",
              "      <td>0.762400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>305</td>\n",
              "      <td>0.710300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>306</td>\n",
              "      <td>0.626600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>307</td>\n",
              "      <td>0.660200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>0.588100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>309</td>\n",
              "      <td>0.727100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.663400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>311</td>\n",
              "      <td>0.619500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>312</td>\n",
              "      <td>0.678300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>313</td>\n",
              "      <td>0.598600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>314</td>\n",
              "      <td>0.598800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>315</td>\n",
              "      <td>0.584700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>316</td>\n",
              "      <td>0.657200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>317</td>\n",
              "      <td>0.670800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>318</td>\n",
              "      <td>0.705500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>319</td>\n",
              "      <td>0.731200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.692300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>321</td>\n",
              "      <td>0.609200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>322</td>\n",
              "      <td>0.700200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>323</td>\n",
              "      <td>0.580600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>324</td>\n",
              "      <td>0.630800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.692700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>326</td>\n",
              "      <td>0.774200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>327</td>\n",
              "      <td>0.726100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>328</td>\n",
              "      <td>0.671000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>329</td>\n",
              "      <td>0.600300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.650900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>331</td>\n",
              "      <td>0.579500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>332</td>\n",
              "      <td>0.677200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>333</td>\n",
              "      <td>0.733100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>334</td>\n",
              "      <td>0.735500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>335</td>\n",
              "      <td>0.710400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.735300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>337</td>\n",
              "      <td>0.704100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>338</td>\n",
              "      <td>0.729200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>339</td>\n",
              "      <td>0.677100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.625300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>341</td>\n",
              "      <td>0.663100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>0.601300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>343</td>\n",
              "      <td>0.579100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>344</td>\n",
              "      <td>0.664500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>0.630900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>346</td>\n",
              "      <td>0.725000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>347</td>\n",
              "      <td>0.597100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>348</td>\n",
              "      <td>0.695500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>349</td>\n",
              "      <td>0.683700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.677300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>351</td>\n",
              "      <td>0.681900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>352</td>\n",
              "      <td>0.627500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>353</td>\n",
              "      <td>0.696300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>354</td>\n",
              "      <td>0.732700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>355</td>\n",
              "      <td>0.636400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>356</td>\n",
              "      <td>0.587100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>357</td>\n",
              "      <td>0.622700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>358</td>\n",
              "      <td>0.628200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>359</td>\n",
              "      <td>0.631300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.724300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>361</td>\n",
              "      <td>0.551000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>362</td>\n",
              "      <td>0.672200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>363</td>\n",
              "      <td>0.585300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>0.774200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>365</td>\n",
              "      <td>0.691200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>366</td>\n",
              "      <td>0.694500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>367</td>\n",
              "      <td>0.611800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>368</td>\n",
              "      <td>0.630800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>369</td>\n",
              "      <td>0.605400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.610100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>371</td>\n",
              "      <td>0.610300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>372</td>\n",
              "      <td>0.613300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>373</td>\n",
              "      <td>0.653900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>374</td>\n",
              "      <td>0.664300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.632100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>376</td>\n",
              "      <td>0.882100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>377</td>\n",
              "      <td>0.690300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>378</td>\n",
              "      <td>0.644800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>379</td>\n",
              "      <td>0.551300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.575600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>381</td>\n",
              "      <td>0.575000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>382</td>\n",
              "      <td>0.700400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>383</td>\n",
              "      <td>0.576300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>384</td>\n",
              "      <td>0.756500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>385</td>\n",
              "      <td>0.686000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>386</td>\n",
              "      <td>0.588700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>387</td>\n",
              "      <td>0.724600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>388</td>\n",
              "      <td>0.540500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>389</td>\n",
              "      <td>0.709000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.625600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>391</td>\n",
              "      <td>0.619600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>0.590700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>393</td>\n",
              "      <td>0.647300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>394</td>\n",
              "      <td>0.643700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>395</td>\n",
              "      <td>0.604600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>396</td>\n",
              "      <td>0.696100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>397</td>\n",
              "      <td>0.745500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>398</td>\n",
              "      <td>0.591800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>0.647200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.602800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>401</td>\n",
              "      <td>0.699100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>402</td>\n",
              "      <td>0.570500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>403</td>\n",
              "      <td>0.776800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>404</td>\n",
              "      <td>0.531300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>405</td>\n",
              "      <td>0.652300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>406</td>\n",
              "      <td>0.582300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>407</td>\n",
              "      <td>0.690200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>408</td>\n",
              "      <td>0.561300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>409</td>\n",
              "      <td>0.722400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.616100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>411</td>\n",
              "      <td>0.450400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>412</td>\n",
              "      <td>0.612600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>413</td>\n",
              "      <td>0.531900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>414</td>\n",
              "      <td>0.739800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>415</td>\n",
              "      <td>0.702300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>416</td>\n",
              "      <td>0.553600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>417</td>\n",
              "      <td>0.740800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>418</td>\n",
              "      <td>0.684000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>419</td>\n",
              "      <td>0.660600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.594600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>421</td>\n",
              "      <td>0.557900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>422</td>\n",
              "      <td>0.699200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>423</td>\n",
              "      <td>0.624400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>424</td>\n",
              "      <td>0.542700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>0.654700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>426</td>\n",
              "      <td>0.618500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>427</td>\n",
              "      <td>0.634100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>428</td>\n",
              "      <td>0.700300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>429</td>\n",
              "      <td>0.733300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.539500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>431</td>\n",
              "      <td>0.644200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>432</td>\n",
              "      <td>0.714600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>433</td>\n",
              "      <td>0.553900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>434</td>\n",
              "      <td>0.737500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>435</td>\n",
              "      <td>0.629900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>436</td>\n",
              "      <td>0.674000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>437</td>\n",
              "      <td>0.622500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>438</td>\n",
              "      <td>0.645900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>439</td>\n",
              "      <td>0.615600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.619000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>441</td>\n",
              "      <td>0.642000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>442</td>\n",
              "      <td>0.723800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>443</td>\n",
              "      <td>0.737400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>444</td>\n",
              "      <td>0.556300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>445</td>\n",
              "      <td>0.614100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>446</td>\n",
              "      <td>0.624200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>447</td>\n",
              "      <td>0.612200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>448</td>\n",
              "      <td>0.539300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>449</td>\n",
              "      <td>0.585000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.724400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>451</td>\n",
              "      <td>0.704300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>452</td>\n",
              "      <td>0.556800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>453</td>\n",
              "      <td>0.614800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>454</td>\n",
              "      <td>0.595200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>455</td>\n",
              "      <td>0.620400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>456</td>\n",
              "      <td>0.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>457</td>\n",
              "      <td>0.639200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>458</td>\n",
              "      <td>0.636700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>459</td>\n",
              "      <td>0.665400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.646500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>461</td>\n",
              "      <td>0.670600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>462</td>\n",
              "      <td>0.595700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>463</td>\n",
              "      <td>0.561100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>464</td>\n",
              "      <td>0.634900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>465</td>\n",
              "      <td>0.634900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>466</td>\n",
              "      <td>0.559800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>467</td>\n",
              "      <td>0.572800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>468</td>\n",
              "      <td>0.679500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>469</td>\n",
              "      <td>0.835800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.601300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>471</td>\n",
              "      <td>0.511900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>472</td>\n",
              "      <td>0.603000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>473</td>\n",
              "      <td>0.743400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>474</td>\n",
              "      <td>0.694900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>0.582300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>476</td>\n",
              "      <td>0.578200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>477</td>\n",
              "      <td>0.635200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>478</td>\n",
              "      <td>0.598200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>479</td>\n",
              "      <td>0.636100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.558200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>481</td>\n",
              "      <td>0.667200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>482</td>\n",
              "      <td>0.661900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>483</td>\n",
              "      <td>0.564300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>484</td>\n",
              "      <td>0.605500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>485</td>\n",
              "      <td>0.659800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>486</td>\n",
              "      <td>0.639900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>487</td>\n",
              "      <td>0.678200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>488</td>\n",
              "      <td>0.528800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>489</td>\n",
              "      <td>0.644600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.627100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>491</td>\n",
              "      <td>0.659800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>492</td>\n",
              "      <td>0.582100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>493</td>\n",
              "      <td>0.626600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>494</td>\n",
              "      <td>0.658000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>495</td>\n",
              "      <td>0.542000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>496</td>\n",
              "      <td>0.514700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>497</td>\n",
              "      <td>0.597400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>498</td>\n",
              "      <td>0.672600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>499</td>\n",
              "      <td>0.709700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.663000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>501</td>\n",
              "      <td>0.583000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>502</td>\n",
              "      <td>0.621900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>503</td>\n",
              "      <td>0.733200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>504</td>\n",
              "      <td>0.711600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>505</td>\n",
              "      <td>0.590300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>506</td>\n",
              "      <td>0.703800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>507</td>\n",
              "      <td>0.615400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>508</td>\n",
              "      <td>0.584600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>509</td>\n",
              "      <td>0.651200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.685500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>511</td>\n",
              "      <td>0.635200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>512</td>\n",
              "      <td>0.635000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>513</td>\n",
              "      <td>0.650600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>514</td>\n",
              "      <td>0.745200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>515</td>\n",
              "      <td>0.621200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>516</td>\n",
              "      <td>0.672900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>517</td>\n",
              "      <td>0.792600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>518</td>\n",
              "      <td>0.588500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>519</td>\n",
              "      <td>0.733100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.620700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>521</td>\n",
              "      <td>0.603600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>522</td>\n",
              "      <td>0.605300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>523</td>\n",
              "      <td>0.760400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>524</td>\n",
              "      <td>0.686900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>525</td>\n",
              "      <td>0.604700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>526</td>\n",
              "      <td>0.676200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>527</td>\n",
              "      <td>0.699900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>528</td>\n",
              "      <td>0.713800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>529</td>\n",
              "      <td>0.560600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.714400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>531</td>\n",
              "      <td>0.649900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>532</td>\n",
              "      <td>0.630900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>533</td>\n",
              "      <td>0.656600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>534</td>\n",
              "      <td>0.694200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>535</td>\n",
              "      <td>0.566000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>536</td>\n",
              "      <td>0.630400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>537</td>\n",
              "      <td>0.646800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>538</td>\n",
              "      <td>0.641000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>539</td>\n",
              "      <td>0.820900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.569700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>541</td>\n",
              "      <td>0.477900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>542</td>\n",
              "      <td>0.655900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>543</td>\n",
              "      <td>0.639500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>544</td>\n",
              "      <td>0.642400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>545</td>\n",
              "      <td>0.778400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>546</td>\n",
              "      <td>0.595200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>547</td>\n",
              "      <td>0.597400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>548</td>\n",
              "      <td>0.762000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>549</td>\n",
              "      <td>0.561400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.584800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>551</td>\n",
              "      <td>0.811500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>552</td>\n",
              "      <td>0.592100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>553</td>\n",
              "      <td>0.681600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>554</td>\n",
              "      <td>0.692300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>555</td>\n",
              "      <td>0.586000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>556</td>\n",
              "      <td>0.596100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>557</td>\n",
              "      <td>0.554000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>558</td>\n",
              "      <td>0.599800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>559</td>\n",
              "      <td>0.695400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.632800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>561</td>\n",
              "      <td>0.601300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>562</td>\n",
              "      <td>0.727900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>563</td>\n",
              "      <td>0.690900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>564</td>\n",
              "      <td>0.560600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>565</td>\n",
              "      <td>0.707800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>566</td>\n",
              "      <td>0.626700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>567</td>\n",
              "      <td>0.702900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>568</td>\n",
              "      <td>0.811700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>569</td>\n",
              "      <td>0.635900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.676600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>571</td>\n",
              "      <td>0.588200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>572</td>\n",
              "      <td>0.631300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>573</td>\n",
              "      <td>0.592700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>574</td>\n",
              "      <td>0.663000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>0.648100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>576</td>\n",
              "      <td>0.531100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>577</td>\n",
              "      <td>0.644900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>578</td>\n",
              "      <td>0.606200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>579</td>\n",
              "      <td>0.625300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.629000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>581</td>\n",
              "      <td>0.676300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>582</td>\n",
              "      <td>0.595500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>583</td>\n",
              "      <td>0.662200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>584</td>\n",
              "      <td>0.591000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>585</td>\n",
              "      <td>0.612300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>586</td>\n",
              "      <td>0.619200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>587</td>\n",
              "      <td>0.666700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>588</td>\n",
              "      <td>0.699800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>589</td>\n",
              "      <td>0.698000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.658600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>591</td>\n",
              "      <td>0.681800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>592</td>\n",
              "      <td>0.729500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>593</td>\n",
              "      <td>0.709000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>594</td>\n",
              "      <td>0.696900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>595</td>\n",
              "      <td>0.664000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>596</td>\n",
              "      <td>0.607700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>597</td>\n",
              "      <td>0.558600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>598</td>\n",
              "      <td>0.706600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>599</td>\n",
              "      <td>0.665300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.646800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>601</td>\n",
              "      <td>0.690500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>602</td>\n",
              "      <td>0.676800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>603</td>\n",
              "      <td>0.562100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>604</td>\n",
              "      <td>0.631200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>605</td>\n",
              "      <td>0.546100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>606</td>\n",
              "      <td>0.779600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>607</td>\n",
              "      <td>0.665600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>608</td>\n",
              "      <td>0.600100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>609</td>\n",
              "      <td>0.557200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.702700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>611</td>\n",
              "      <td>0.621400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>612</td>\n",
              "      <td>0.672000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>613</td>\n",
              "      <td>0.714200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>614</td>\n",
              "      <td>0.641500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>615</td>\n",
              "      <td>0.610700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>616</td>\n",
              "      <td>0.595700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>617</td>\n",
              "      <td>0.653700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>618</td>\n",
              "      <td>0.605800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>619</td>\n",
              "      <td>0.742100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.565500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>621</td>\n",
              "      <td>0.628500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>622</td>\n",
              "      <td>0.614000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>623</td>\n",
              "      <td>0.687500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>624</td>\n",
              "      <td>0.552100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>0.600200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>626</td>\n",
              "      <td>0.693200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>627</td>\n",
              "      <td>0.693400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>628</td>\n",
              "      <td>0.644000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>629</td>\n",
              "      <td>0.627400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.612800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>631</td>\n",
              "      <td>0.655400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>632</td>\n",
              "      <td>0.520400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>633</td>\n",
              "      <td>0.662600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>634</td>\n",
              "      <td>0.633600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>635</td>\n",
              "      <td>0.641200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>636</td>\n",
              "      <td>0.676300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>637</td>\n",
              "      <td>0.646200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>638</td>\n",
              "      <td>0.614200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>639</td>\n",
              "      <td>0.709900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.632000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>641</td>\n",
              "      <td>0.637000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>642</td>\n",
              "      <td>0.789400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>643</td>\n",
              "      <td>0.639400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>644</td>\n",
              "      <td>0.515800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>645</td>\n",
              "      <td>0.670800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>646</td>\n",
              "      <td>0.578300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>647</td>\n",
              "      <td>0.736400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>648</td>\n",
              "      <td>0.658500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>649</td>\n",
              "      <td>0.702500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.699900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>651</td>\n",
              "      <td>0.630800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>652</td>\n",
              "      <td>0.538800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>653</td>\n",
              "      <td>0.634200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>654</td>\n",
              "      <td>0.504900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>655</td>\n",
              "      <td>0.628500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>656</td>\n",
              "      <td>0.758400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>657</td>\n",
              "      <td>0.698300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>658</td>\n",
              "      <td>0.734500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>659</td>\n",
              "      <td>0.660400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.657200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>661</td>\n",
              "      <td>0.646500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>662</td>\n",
              "      <td>0.592100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>663</td>\n",
              "      <td>0.494300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>664</td>\n",
              "      <td>0.531600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>665</td>\n",
              "      <td>0.579200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>666</td>\n",
              "      <td>0.588400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>667</td>\n",
              "      <td>0.662400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>668</td>\n",
              "      <td>0.572800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>669</td>\n",
              "      <td>0.711700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>0.612500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>671</td>\n",
              "      <td>0.745700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>672</td>\n",
              "      <td>0.628200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>673</td>\n",
              "      <td>0.742600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>674</td>\n",
              "      <td>0.593000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>675</td>\n",
              "      <td>0.712100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>676</td>\n",
              "      <td>0.591600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>677</td>\n",
              "      <td>0.522700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>678</td>\n",
              "      <td>0.655500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>679</td>\n",
              "      <td>0.635200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.468000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>681</td>\n",
              "      <td>0.653400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>682</td>\n",
              "      <td>0.551500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>683</td>\n",
              "      <td>0.648600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>684</td>\n",
              "      <td>0.597800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>685</td>\n",
              "      <td>0.626600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>686</td>\n",
              "      <td>0.575900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>687</td>\n",
              "      <td>0.620400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>688</td>\n",
              "      <td>0.592600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>689</td>\n",
              "      <td>0.721500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.595200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>691</td>\n",
              "      <td>0.723700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>692</td>\n",
              "      <td>0.675800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>693</td>\n",
              "      <td>0.597100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>694</td>\n",
              "      <td>0.752200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>695</td>\n",
              "      <td>0.585500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>696</td>\n",
              "      <td>0.743600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>697</td>\n",
              "      <td>0.691500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>698</td>\n",
              "      <td>0.729700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>699</td>\n",
              "      <td>0.678100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.523200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>701</td>\n",
              "      <td>0.623000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>702</td>\n",
              "      <td>0.564300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>703</td>\n",
              "      <td>0.763800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>704</td>\n",
              "      <td>0.541600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>705</td>\n",
              "      <td>0.788600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>706</td>\n",
              "      <td>0.534500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>707</td>\n",
              "      <td>0.616100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>708</td>\n",
              "      <td>0.599300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>709</td>\n",
              "      <td>0.574300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>0.585200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>711</td>\n",
              "      <td>0.723100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>712</td>\n",
              "      <td>0.527600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>713</td>\n",
              "      <td>0.577900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>714</td>\n",
              "      <td>0.737200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>715</td>\n",
              "      <td>0.600500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>716</td>\n",
              "      <td>0.599900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>717</td>\n",
              "      <td>0.599200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>718</td>\n",
              "      <td>0.639100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>719</td>\n",
              "      <td>0.617400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>0.582000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>721</td>\n",
              "      <td>0.489300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>722</td>\n",
              "      <td>0.531800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>723</td>\n",
              "      <td>0.635000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>724</td>\n",
              "      <td>0.676800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>725</td>\n",
              "      <td>0.660500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>726</td>\n",
              "      <td>0.517300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>727</td>\n",
              "      <td>0.639100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>728</td>\n",
              "      <td>0.504400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>729</td>\n",
              "      <td>0.603200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>0.651800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>731</td>\n",
              "      <td>0.681700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>732</td>\n",
              "      <td>0.787200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>733</td>\n",
              "      <td>0.569300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>734</td>\n",
              "      <td>0.669000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>735</td>\n",
              "      <td>0.568700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>736</td>\n",
              "      <td>0.751100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>737</td>\n",
              "      <td>0.586700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>738</td>\n",
              "      <td>0.681200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>739</td>\n",
              "      <td>0.606300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.609700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>741</td>\n",
              "      <td>0.741400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>742</td>\n",
              "      <td>0.539800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>743</td>\n",
              "      <td>0.717300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>744</td>\n",
              "      <td>0.634900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>745</td>\n",
              "      <td>0.663700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>746</td>\n",
              "      <td>0.568400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>747</td>\n",
              "      <td>0.583000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>748</td>\n",
              "      <td>0.674400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>749</td>\n",
              "      <td>0.683600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.667300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>751</td>\n",
              "      <td>0.658800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>752</td>\n",
              "      <td>0.633000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>753</td>\n",
              "      <td>0.721000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>754</td>\n",
              "      <td>0.789100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>755</td>\n",
              "      <td>0.531300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>756</td>\n",
              "      <td>0.739400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>757</td>\n",
              "      <td>0.539300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>758</td>\n",
              "      <td>0.578700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>759</td>\n",
              "      <td>0.709300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.589400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>761</td>\n",
              "      <td>0.590200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>762</td>\n",
              "      <td>0.763500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>763</td>\n",
              "      <td>0.628900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>764</td>\n",
              "      <td>0.611200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>765</td>\n",
              "      <td>0.631100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>766</td>\n",
              "      <td>0.580200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>767</td>\n",
              "      <td>0.485500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>768</td>\n",
              "      <td>0.581800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>769</td>\n",
              "      <td>0.572700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>0.489100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>771</td>\n",
              "      <td>0.580800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>772</td>\n",
              "      <td>0.733900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>773</td>\n",
              "      <td>0.610700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>774</td>\n",
              "      <td>0.708600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>775</td>\n",
              "      <td>0.582000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>776</td>\n",
              "      <td>0.728000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>777</td>\n",
              "      <td>0.497300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>778</td>\n",
              "      <td>0.819200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>779</td>\n",
              "      <td>0.588200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>0.599600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>781</td>\n",
              "      <td>0.600400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>782</td>\n",
              "      <td>0.649600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>783</td>\n",
              "      <td>0.596900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>784</td>\n",
              "      <td>0.557500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>785</td>\n",
              "      <td>0.643100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>786</td>\n",
              "      <td>0.620600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>787</td>\n",
              "      <td>0.615200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>788</td>\n",
              "      <td>0.580100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>789</td>\n",
              "      <td>0.717400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>0.580300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>791</td>\n",
              "      <td>0.575600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>792</td>\n",
              "      <td>0.682300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>793</td>\n",
              "      <td>0.573000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>794</td>\n",
              "      <td>0.552000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>795</td>\n",
              "      <td>0.643800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>796</td>\n",
              "      <td>0.497400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>797</td>\n",
              "      <td>0.615900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>798</td>\n",
              "      <td>0.506800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>799</td>\n",
              "      <td>0.601400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.659800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>801</td>\n",
              "      <td>0.651400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>802</td>\n",
              "      <td>0.562200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>803</td>\n",
              "      <td>0.606000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>804</td>\n",
              "      <td>0.733900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>805</td>\n",
              "      <td>0.574300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>806</td>\n",
              "      <td>0.672200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>807</td>\n",
              "      <td>0.706900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>808</td>\n",
              "      <td>0.691100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>809</td>\n",
              "      <td>0.730600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>0.512000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>811</td>\n",
              "      <td>0.664300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>812</td>\n",
              "      <td>0.688500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>813</td>\n",
              "      <td>0.556200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>814</td>\n",
              "      <td>0.723700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>815</td>\n",
              "      <td>0.791600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>816</td>\n",
              "      <td>0.712000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>817</td>\n",
              "      <td>0.615100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>818</td>\n",
              "      <td>0.658000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>819</td>\n",
              "      <td>0.639100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>0.639200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>821</td>\n",
              "      <td>0.541200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>822</td>\n",
              "      <td>0.758400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>823</td>\n",
              "      <td>0.654500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>824</td>\n",
              "      <td>0.682500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>825</td>\n",
              "      <td>0.535200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>826</td>\n",
              "      <td>0.668400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>827</td>\n",
              "      <td>0.619600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>828</td>\n",
              "      <td>0.758600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>829</td>\n",
              "      <td>0.577600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>0.528600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>831</td>\n",
              "      <td>0.615800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>832</td>\n",
              "      <td>0.641700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>833</td>\n",
              "      <td>0.708300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>834</td>\n",
              "      <td>0.598400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>835</td>\n",
              "      <td>0.660300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>836</td>\n",
              "      <td>0.617400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>837</td>\n",
              "      <td>0.637300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>838</td>\n",
              "      <td>0.539900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>839</td>\n",
              "      <td>0.595300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>0.559200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>841</td>\n",
              "      <td>0.584600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>842</td>\n",
              "      <td>0.688400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>843</td>\n",
              "      <td>0.613000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>844</td>\n",
              "      <td>0.723400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>845</td>\n",
              "      <td>0.501100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>846</td>\n",
              "      <td>0.717100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>847</td>\n",
              "      <td>0.599200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>848</td>\n",
              "      <td>0.651000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>849</td>\n",
              "      <td>0.674800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.767900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>851</td>\n",
              "      <td>0.698200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>852</td>\n",
              "      <td>0.705000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>853</td>\n",
              "      <td>0.648400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>854</td>\n",
              "      <td>0.538400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>855</td>\n",
              "      <td>0.651500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>856</td>\n",
              "      <td>0.590200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>857</td>\n",
              "      <td>0.432800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>858</td>\n",
              "      <td>0.703400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>859</td>\n",
              "      <td>0.652300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>0.518300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>861</td>\n",
              "      <td>0.649800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>862</td>\n",
              "      <td>0.596000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>863</td>\n",
              "      <td>0.460400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>864</td>\n",
              "      <td>0.619700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>865</td>\n",
              "      <td>0.637600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>866</td>\n",
              "      <td>0.653900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>867</td>\n",
              "      <td>0.617100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>868</td>\n",
              "      <td>0.724100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>869</td>\n",
              "      <td>0.624600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>0.582700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>871</td>\n",
              "      <td>0.665200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>872</td>\n",
              "      <td>0.608600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>873</td>\n",
              "      <td>0.616900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>874</td>\n",
              "      <td>0.662500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>875</td>\n",
              "      <td>0.594600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>876</td>\n",
              "      <td>0.528900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>877</td>\n",
              "      <td>0.679400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>878</td>\n",
              "      <td>0.675400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>879</td>\n",
              "      <td>0.572400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>0.715600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>881</td>\n",
              "      <td>0.517200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>882</td>\n",
              "      <td>0.621800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>883</td>\n",
              "      <td>0.617700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>884</td>\n",
              "      <td>0.689100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>885</td>\n",
              "      <td>0.589300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>886</td>\n",
              "      <td>0.636600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>887</td>\n",
              "      <td>0.559700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>888</td>\n",
              "      <td>0.656200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>889</td>\n",
              "      <td>0.516600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>0.697900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>891</td>\n",
              "      <td>0.659400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>892</td>\n",
              "      <td>0.591600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>893</td>\n",
              "      <td>0.555600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>894</td>\n",
              "      <td>0.610100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>895</td>\n",
              "      <td>0.613400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>896</td>\n",
              "      <td>0.622300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>897</td>\n",
              "      <td>0.644800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>898</td>\n",
              "      <td>0.678900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>899</td>\n",
              "      <td>0.586800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.586800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>901</td>\n",
              "      <td>0.571700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>902</td>\n",
              "      <td>0.662400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>903</td>\n",
              "      <td>0.561200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>904</td>\n",
              "      <td>0.692900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>905</td>\n",
              "      <td>0.625700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>906</td>\n",
              "      <td>0.597100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>907</td>\n",
              "      <td>0.552700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>908</td>\n",
              "      <td>0.753400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>909</td>\n",
              "      <td>0.636300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>0.612500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>911</td>\n",
              "      <td>0.584100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>912</td>\n",
              "      <td>0.530500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>913</td>\n",
              "      <td>0.583000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>914</td>\n",
              "      <td>0.566500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>915</td>\n",
              "      <td>0.596600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>916</td>\n",
              "      <td>0.657500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>917</td>\n",
              "      <td>0.593500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>918</td>\n",
              "      <td>0.883800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>919</td>\n",
              "      <td>0.571400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.615300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>921</td>\n",
              "      <td>0.513000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>922</td>\n",
              "      <td>0.603400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>923</td>\n",
              "      <td>0.672000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>924</td>\n",
              "      <td>0.740700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>925</td>\n",
              "      <td>0.533000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>926</td>\n",
              "      <td>0.635900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>927</td>\n",
              "      <td>0.621000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>928</td>\n",
              "      <td>0.698100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>929</td>\n",
              "      <td>0.650600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>0.568700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>931</td>\n",
              "      <td>0.561000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>932</td>\n",
              "      <td>0.524100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>933</td>\n",
              "      <td>0.669100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>934</td>\n",
              "      <td>0.589500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>935</td>\n",
              "      <td>0.594700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>936</td>\n",
              "      <td>0.692100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>937</td>\n",
              "      <td>0.622200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>938</td>\n",
              "      <td>0.544100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>939</td>\n",
              "      <td>0.721800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>0.666600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>941</td>\n",
              "      <td>0.613200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>942</td>\n",
              "      <td>0.613700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>943</td>\n",
              "      <td>0.579300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>944</td>\n",
              "      <td>0.536300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>945</td>\n",
              "      <td>0.649700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>946</td>\n",
              "      <td>0.587500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>947</td>\n",
              "      <td>0.589100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>948</td>\n",
              "      <td>0.642200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>949</td>\n",
              "      <td>0.700200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.596900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>951</td>\n",
              "      <td>0.582400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>952</td>\n",
              "      <td>0.659400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>953</td>\n",
              "      <td>0.637800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>954</td>\n",
              "      <td>0.539200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>955</td>\n",
              "      <td>0.548500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>956</td>\n",
              "      <td>0.625100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>957</td>\n",
              "      <td>0.594900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>958</td>\n",
              "      <td>0.556300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>959</td>\n",
              "      <td>0.703000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>0.669700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>961</td>\n",
              "      <td>0.643300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>962</td>\n",
              "      <td>0.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>963</td>\n",
              "      <td>0.569100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>964</td>\n",
              "      <td>0.608000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>965</td>\n",
              "      <td>0.638900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>966</td>\n",
              "      <td>0.647900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>967</td>\n",
              "      <td>0.552100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>968</td>\n",
              "      <td>0.711300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>969</td>\n",
              "      <td>0.673400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>0.655200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>971</td>\n",
              "      <td>0.553300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>972</td>\n",
              "      <td>0.608600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>973</td>\n",
              "      <td>0.568000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>974</td>\n",
              "      <td>0.596200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>975</td>\n",
              "      <td>0.551200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>976</td>\n",
              "      <td>0.808200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>977</td>\n",
              "      <td>0.593000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>978</td>\n",
              "      <td>0.641500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>979</td>\n",
              "      <td>0.612800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>0.621600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>981</td>\n",
              "      <td>0.631400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>982</td>\n",
              "      <td>0.629100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>983</td>\n",
              "      <td>0.627900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>984</td>\n",
              "      <td>0.564900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>985</td>\n",
              "      <td>0.716800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>986</td>\n",
              "      <td>0.694500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>987</td>\n",
              "      <td>0.660300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>988</td>\n",
              "      <td>0.637500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>989</td>\n",
              "      <td>0.653100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>990</td>\n",
              "      <td>0.715100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>991</td>\n",
              "      <td>0.705800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>992</td>\n",
              "      <td>0.548000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>993</td>\n",
              "      <td>0.738200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>994</td>\n",
              "      <td>0.545300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>995</td>\n",
              "      <td>0.740400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>996</td>\n",
              "      <td>0.636500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>997</td>\n",
              "      <td>0.615400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>998</td>\n",
              "      <td>0.607100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>999</td>\n",
              "      <td>0.654800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.605500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1001</td>\n",
              "      <td>0.622800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1002</td>\n",
              "      <td>0.611700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1003</td>\n",
              "      <td>0.581100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1004</td>\n",
              "      <td>0.520500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1005</td>\n",
              "      <td>0.762900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1006</td>\n",
              "      <td>0.624800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1007</td>\n",
              "      <td>0.624100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1008</td>\n",
              "      <td>0.629100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1009</td>\n",
              "      <td>0.632800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1010</td>\n",
              "      <td>0.616500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1011</td>\n",
              "      <td>0.653500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1012</td>\n",
              "      <td>0.660200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1013</td>\n",
              "      <td>0.770300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1014</td>\n",
              "      <td>0.633400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1015</td>\n",
              "      <td>0.475800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1016</td>\n",
              "      <td>0.803300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1017</td>\n",
              "      <td>0.620600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1018</td>\n",
              "      <td>0.657800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1019</td>\n",
              "      <td>0.667000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>0.643900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1021</td>\n",
              "      <td>0.674400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1022</td>\n",
              "      <td>0.528800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1023</td>\n",
              "      <td>0.626600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1024</td>\n",
              "      <td>0.546200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1025</td>\n",
              "      <td>0.685200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1026</td>\n",
              "      <td>0.627400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1027</td>\n",
              "      <td>0.653400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1028</td>\n",
              "      <td>0.692200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1029</td>\n",
              "      <td>0.665100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1030</td>\n",
              "      <td>0.680400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1031</td>\n",
              "      <td>0.611200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1032</td>\n",
              "      <td>0.634600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1033</td>\n",
              "      <td>0.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1034</td>\n",
              "      <td>0.658800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1035</td>\n",
              "      <td>0.610000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1036</td>\n",
              "      <td>0.602300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1037</td>\n",
              "      <td>0.626900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1038</td>\n",
              "      <td>0.585900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1039</td>\n",
              "      <td>0.577700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>0.597100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1041</td>\n",
              "      <td>0.614900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1042</td>\n",
              "      <td>0.608600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1043</td>\n",
              "      <td>0.597400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1044</td>\n",
              "      <td>0.578800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1045</td>\n",
              "      <td>0.592300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1046</td>\n",
              "      <td>0.601700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1047</td>\n",
              "      <td>0.559800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1048</td>\n",
              "      <td>0.720500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1049</td>\n",
              "      <td>0.722100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.563100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1051</td>\n",
              "      <td>0.495500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1052</td>\n",
              "      <td>0.728200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1053</td>\n",
              "      <td>0.799100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1054</td>\n",
              "      <td>0.579500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1055</td>\n",
              "      <td>0.639500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1056</td>\n",
              "      <td>0.690600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1057</td>\n",
              "      <td>0.584100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1058</td>\n",
              "      <td>0.628500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1059</td>\n",
              "      <td>0.680400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>0.583700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1061</td>\n",
              "      <td>0.548100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1062</td>\n",
              "      <td>0.619800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1063</td>\n",
              "      <td>0.566800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1064</td>\n",
              "      <td>0.747400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1065</td>\n",
              "      <td>0.712700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1066</td>\n",
              "      <td>0.660900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1067</td>\n",
              "      <td>0.659300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1068</td>\n",
              "      <td>0.506700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1069</td>\n",
              "      <td>0.701000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1070</td>\n",
              "      <td>0.699800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1071</td>\n",
              "      <td>0.669100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1072</td>\n",
              "      <td>0.620800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1073</td>\n",
              "      <td>0.606900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1074</td>\n",
              "      <td>0.525600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1075</td>\n",
              "      <td>0.591400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1076</td>\n",
              "      <td>0.576600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1077</td>\n",
              "      <td>0.722400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1078</td>\n",
              "      <td>0.661500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1079</td>\n",
              "      <td>0.586400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>0.571600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1081</td>\n",
              "      <td>0.620500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1082</td>\n",
              "      <td>0.709600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1083</td>\n",
              "      <td>0.612400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1084</td>\n",
              "      <td>0.645200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1085</td>\n",
              "      <td>0.536800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1086</td>\n",
              "      <td>0.624700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1087</td>\n",
              "      <td>0.745100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1088</td>\n",
              "      <td>0.706000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1089</td>\n",
              "      <td>0.666300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1090</td>\n",
              "      <td>0.599800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1091</td>\n",
              "      <td>0.573400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1092</td>\n",
              "      <td>0.536600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1093</td>\n",
              "      <td>0.670100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1094</td>\n",
              "      <td>0.757400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1095</td>\n",
              "      <td>0.572700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1096</td>\n",
              "      <td>0.625400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1097</td>\n",
              "      <td>0.689400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1098</td>\n",
              "      <td>0.665100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1099</td>\n",
              "      <td>0.621300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.605600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1101</td>\n",
              "      <td>0.535700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1102</td>\n",
              "      <td>0.741000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1103</td>\n",
              "      <td>0.720100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1104</td>\n",
              "      <td>0.597900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1105</td>\n",
              "      <td>0.667700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1106</td>\n",
              "      <td>0.632900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1107</td>\n",
              "      <td>0.538900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1108</td>\n",
              "      <td>0.680200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1109</td>\n",
              "      <td>0.754500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1110</td>\n",
              "      <td>0.582200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1111</td>\n",
              "      <td>0.552700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1112</td>\n",
              "      <td>0.740500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1113</td>\n",
              "      <td>0.688800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1114</td>\n",
              "      <td>0.595100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1115</td>\n",
              "      <td>0.635300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1116</td>\n",
              "      <td>0.523300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1117</td>\n",
              "      <td>0.603300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1118</td>\n",
              "      <td>0.614800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1119</td>\n",
              "      <td>0.532600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1120</td>\n",
              "      <td>0.657400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1121</td>\n",
              "      <td>0.603800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1122</td>\n",
              "      <td>0.646900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1123</td>\n",
              "      <td>0.629400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1124</td>\n",
              "      <td>0.683500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1125</td>\n",
              "      <td>0.467400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1126</td>\n",
              "      <td>0.685200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1127</td>\n",
              "      <td>0.515800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1128</td>\n",
              "      <td>0.669300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1129</td>\n",
              "      <td>0.546500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1130</td>\n",
              "      <td>0.653900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1131</td>\n",
              "      <td>0.648400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1132</td>\n",
              "      <td>0.672400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1133</td>\n",
              "      <td>0.510100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1134</td>\n",
              "      <td>0.736800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1135</td>\n",
              "      <td>0.575900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1136</td>\n",
              "      <td>0.493200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1137</td>\n",
              "      <td>0.474600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1138</td>\n",
              "      <td>0.660300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1139</td>\n",
              "      <td>0.629900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1140</td>\n",
              "      <td>0.583700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1141</td>\n",
              "      <td>0.579400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1142</td>\n",
              "      <td>0.615100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1143</td>\n",
              "      <td>0.570900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1144</td>\n",
              "      <td>0.613400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1145</td>\n",
              "      <td>0.677500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1146</td>\n",
              "      <td>0.598700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1147</td>\n",
              "      <td>0.736500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1148</td>\n",
              "      <td>0.650700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1149</td>\n",
              "      <td>0.607200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.721800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1151</td>\n",
              "      <td>0.677800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1152</td>\n",
              "      <td>0.615200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1153</td>\n",
              "      <td>0.596900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1154</td>\n",
              "      <td>0.657900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1155</td>\n",
              "      <td>0.542000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1156</td>\n",
              "      <td>0.637900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1157</td>\n",
              "      <td>0.644700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1158</td>\n",
              "      <td>0.521100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1159</td>\n",
              "      <td>0.602000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1160</td>\n",
              "      <td>0.599100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1161</td>\n",
              "      <td>0.612300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1162</td>\n",
              "      <td>0.579800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1163</td>\n",
              "      <td>0.568400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1164</td>\n",
              "      <td>0.540600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1165</td>\n",
              "      <td>0.568600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1166</td>\n",
              "      <td>0.641400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1167</td>\n",
              "      <td>0.587900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1168</td>\n",
              "      <td>0.539400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1169</td>\n",
              "      <td>0.649800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1170</td>\n",
              "      <td>0.599900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1171</td>\n",
              "      <td>0.679100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1172</td>\n",
              "      <td>0.619300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1173</td>\n",
              "      <td>0.618800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1174</td>\n",
              "      <td>0.576100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1175</td>\n",
              "      <td>0.599700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1176</td>\n",
              "      <td>0.528400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1177</td>\n",
              "      <td>0.680100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1178</td>\n",
              "      <td>0.630300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1179</td>\n",
              "      <td>0.710500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1180</td>\n",
              "      <td>0.678100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1181</td>\n",
              "      <td>0.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1182</td>\n",
              "      <td>0.608500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1183</td>\n",
              "      <td>0.557600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1184</td>\n",
              "      <td>0.558200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1185</td>\n",
              "      <td>0.702100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1186</td>\n",
              "      <td>0.709900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1187</td>\n",
              "      <td>0.591500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1188</td>\n",
              "      <td>0.586600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1189</td>\n",
              "      <td>0.583700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1190</td>\n",
              "      <td>0.609000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1191</td>\n",
              "      <td>0.502900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1192</td>\n",
              "      <td>0.664100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1193</td>\n",
              "      <td>0.565800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1194</td>\n",
              "      <td>0.697600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1195</td>\n",
              "      <td>0.490100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1196</td>\n",
              "      <td>0.614300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1197</td>\n",
              "      <td>0.531800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1198</td>\n",
              "      <td>0.595000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1199</td>\n",
              "      <td>0.559900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.587000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1201</td>\n",
              "      <td>0.662800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1202</td>\n",
              "      <td>0.526800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1203</td>\n",
              "      <td>0.673600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1204</td>\n",
              "      <td>0.506100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1205</td>\n",
              "      <td>0.592300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1206</td>\n",
              "      <td>0.603700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1207</td>\n",
              "      <td>0.680900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1208</td>\n",
              "      <td>0.558900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1209</td>\n",
              "      <td>0.691800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1210</td>\n",
              "      <td>0.638900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1211</td>\n",
              "      <td>0.558300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1212</td>\n",
              "      <td>0.546800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1213</td>\n",
              "      <td>0.524900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1214</td>\n",
              "      <td>0.624200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1215</td>\n",
              "      <td>0.592700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1216</td>\n",
              "      <td>0.663200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1217</td>\n",
              "      <td>0.742300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1218</td>\n",
              "      <td>0.573600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1219</td>\n",
              "      <td>0.517000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1220</td>\n",
              "      <td>0.523600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1221</td>\n",
              "      <td>0.703000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1222</td>\n",
              "      <td>0.579500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1223</td>\n",
              "      <td>0.558100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1224</td>\n",
              "      <td>0.670200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1225</td>\n",
              "      <td>0.701600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1226</td>\n",
              "      <td>0.610600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1227</td>\n",
              "      <td>0.744400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1228</td>\n",
              "      <td>0.476800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1229</td>\n",
              "      <td>0.708100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1230</td>\n",
              "      <td>0.598200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1231</td>\n",
              "      <td>0.688400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1232</td>\n",
              "      <td>0.620700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1233</td>\n",
              "      <td>0.681000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1234</td>\n",
              "      <td>0.709000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1235</td>\n",
              "      <td>0.508200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1236</td>\n",
              "      <td>0.667600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1237</td>\n",
              "      <td>0.633300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1238</td>\n",
              "      <td>0.738100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1239</td>\n",
              "      <td>0.609600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1240</td>\n",
              "      <td>0.587600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1241</td>\n",
              "      <td>0.640100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1242</td>\n",
              "      <td>0.637200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1243</td>\n",
              "      <td>0.575700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1244</td>\n",
              "      <td>0.631400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1245</td>\n",
              "      <td>0.621800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test after finetuning\n",
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer([\n",
        "    dataset_test['text'][0]\n",
        "], return_tensors='pt').to(\"cuda\")\n",
        "\n",
        "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ew_GKOPWLly",
        "outputId": "13c6f2af-7c44-45ad-f74f-a0074fcd7638"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "\n",
            "### Instruction:\n",
            "You are a helpful math teacher.\n",
            "\n",
            "### Input:\n",
            "Question: Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
            "\n",
            "### Response:\n",
            "\n",
            "Answer: How many eggs does Janet eat? ** She eats 3 eggs for breakfast every day, so she eats 3 * 7 = <<3*7=21>>21 eggs per week.\n",
            "How many eggs does Janet bake? ** She bakes muffins for her friends every day with 4 eggs, so she bakes 4 * 7 = <<4*7=28>>28 eggs per week.\n",
            "How many eggs does Janet sell? ** She sells the remainder at the farmers' market daily for $2 per fresh duck egg, so she sells 16 - 21 - 28 = <<16-21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer([\n",
        "    dataset_test['text'][1]\n",
        "], return_tensors='pt').to(\"cuda\")\n",
        "\n",
        "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcevYev8WiTO",
        "outputId": "5b8b60c6-fc72-4527-8f09-f333e852dd69"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "\n",
            "### Instruction:\n",
            "You are a helpful math teacher.\n",
            "\n",
            "### Input:\n",
            "Question: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\n",
            "\n",
            "### Response:\n",
            "\n",
            "Answer: How many bolts of white fiber does it take? ** It takes 2/2=<<2/2=1>>1 bolt of white fiber\n",
            "How many bolts of fiber does it take? ** So it takes 2+1=<<2+1=3>>3 bolts of fiber\n",
            "#### 3\n",
            "How many bolts in total does it take? ** It takes 3 bolts of fiber\n",
            "#### 3\n",
            "How many bolts in total does it take? ** It takes 3 bolts of fiber\n",
            "#### 3\n",
            "How many bolts in total does it take? ** It takes 3 bolts of fiber\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer([\n",
        "    dataset_test['text'][2]\n",
        "], return_tensors='pt').to(\"cuda\")\n",
        "\n",
        "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSoe5mA_WuZX",
        "outputId": "cdc36c51-403d-4ef4-fde7-55c79a1ee826"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "\n",
            "### Instruction:\n",
            "You are a helpful math teacher.\n",
            "\n",
            "### Input:\n",
            "Question: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?\n",
            "\n",
            "### Response:\n",
            "\n",
            "Answer: How much did the house increase in value? ** The house increased in value by 80,000*.15=$<<80000*.15=12000>>12,000\n",
            "How much did the house sell for? ** So it sold for 80,000+12,000=$<<80000+12000=92000>>92,000\n",
            "How much did Josh make? ** That means he made 92,000-80,000=$<<92000-80000=12000>>12,000\n",
            "#### 12000\n",
            "How much profit did Josh make? ** So he made 12,000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer([\n",
        "    dataset_test['text'][3]\n",
        "], return_tensors='pt').to(\"cuda\")\n",
        "\n",
        "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2sGgfuWXAPv",
        "outputId": "53f96e27-003a-4306-a192-7b0c38f95442"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "\n",
            "### Instruction:\n",
            "You are a helpful math teacher.\n",
            "\n",
            "### Input:\n",
            "Question: James decides to run 3 sprints 3 times a week.  He runs 60 meters each sprint.  How many total meters does he run a week?\n",
            "\n",
            "### Response:\n",
            "\n",
            "Answer: How many meters does James run each sprint? ** He runs 60 meters each sprint so that's 60*3=<<60*3=180>>180 meters\n",
            "How many meters does James run each week? ** He runs 3 times a week so that's 180*3=<<180*3=540>>540 meters\n",
            "#### 540\n",
            "How many total meters does James run a week? ** He runs 540 meters a week\n",
            "#### 540\n",
            "How many total meters does James run a week? ** He runs 540 meters a week\n",
            "#### 540\n",
            "How many total meters does James\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save LoRA model\n",
        "\n",
        "model.save_pretrained(\"lora_model_gsm8k\") # save to local drive\n",
        "# model.push_to_hub(\"jfzhang/lora_model\", token='xxxx') # save to huggingface"
      ],
      "metadata": {
        "id": "i0rJ6SmYXNqm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge lora model and base model, quantilize to 4-bit, save gguf\n",
        "model.save_pretrained_gguf(\"model_gsm8k\", tokenizer, quantization_method='q4_k_m')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4oBkTsqYAfU",
        "outputId": "871a6925-4e1a-4609-a954-94b145a996e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
            "We shall switch to Pytorch saving, which will take 3 minutes and not 30 minutes.\n",
            "To force `safe_serialization`, set it to `None` instead.\n",
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 5.7G\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 6.13 out of 12.67 RAM for saving.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:01<00:01, 14.71it/s]We will save to Disk and not RAM now.\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:28<00:00,  2.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
            "Unsloth: Saving model_gsm8k/pytorch_model-00001-of-00004.bin...\n",
            "Unsloth: Saving model_gsm8k/pytorch_model-00002-of-00004.bin...\n",
            "Unsloth: Saving model_gsm8k/pytorch_model-00003-of-00004.bin...\n",
            "Unsloth: Saving model_gsm8k/pytorch_model-00004-of-00004.bin...\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Converting llama model. Can use fast conversion = True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to q4_k_m will take 20 minutes.\n",
            " \"-____-\"     In total, you will have to wait around 26 minutes.\n",
            "\n",
            "Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n",
            "Unsloth: [1] Converting model at model_gsm8k into f16 GGUF format.\n",
            "The output location will be ./model_gsm8k-unsloth.F16.gguf\n",
            "This will take 3 minutes...\n",
            "Loading model file model_gsm8k/pytorch_model-00001-of-00004.bin\n",
            "Loading model file model_gsm8k/pytorch_model-00001-of-00004.bin\n",
            "Loading model file model_gsm8k/pytorch_model-00002-of-00004.bin\n",
            "Loading model file model_gsm8k/pytorch_model-00003-of-00004.bin\n",
            "Loading model file model_gsm8k/pytorch_model-00004-of-00004.bin\n",
            "params = Params(n_vocab=128256, n_embd=4096, n_layer=32, n_ctx=8192, n_ff=14336, n_head=32, n_head_kv=8, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=500000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=PosixPath('model_gsm8k'))\n",
            "Loaded vocab file PosixPath('model_gsm8k/tokenizer.json'), type 'bpe'\n",
            "Vocab info: <BpeVocab with 128000 base tokens and 256 added tokens>\n",
            "Special vocab info: <SpecialVocab with 280147 merges, special tokens {'bos': 128000, 'eos': 128001, 'pad': 128001}, add special tokens unset>\n",
            "Permuting layer 0\n",
            "Permuting layer 1\n",
            "Permuting layer 2\n",
            "Permuting layer 3\n",
            "Permuting layer 4\n",
            "Permuting layer 5\n",
            "Permuting layer 6\n",
            "Permuting layer 7\n",
            "Permuting layer 8\n",
            "Permuting layer 9\n",
            "Permuting layer 10\n",
            "Permuting layer 11\n",
            "Permuting layer 12\n",
            "Permuting layer 13\n",
            "Permuting layer 14\n",
            "Permuting layer 15\n",
            "Permuting layer 16\n",
            "Permuting layer 17\n",
            "Permuting layer 18\n",
            "Permuting layer 19\n",
            "Permuting layer 20\n",
            "Permuting layer 21\n",
            "Permuting layer 22\n",
            "Permuting layer 23\n",
            "Permuting layer 24\n",
            "Permuting layer 25\n",
            "Permuting layer 26\n",
            "Permuting layer 27\n",
            "Permuting layer 28\n",
            "Permuting layer 29\n",
            "Permuting layer 30\n",
            "Permuting layer 31\n",
            "model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [128256, 4096]\n",
            "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [1024, 4096]\n",
            "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [1024, 4096]\n",
            "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [14336, 4096]\n",
            "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [14336, 4096]\n",
            "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [4096, 14336]\n",
            "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [1024, 4096]\n",
            "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [1024, 4096]\n",
            "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [14336, 4096]\n",
            "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [14336, 4096]\n",
            "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [4096, 14336]\n",
            "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [1024, 4096]\n",
            "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [1024, 4096]\n",
            "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [14336, 4096]\n",
            "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [14336, 4096]\n",
            "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [4096, 14336]\n",
            "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [1024, 4096]\n",
            "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [1024, 4096]\n",
            "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [14336, 4096]\n",
            "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [14336, 4096]\n",
            "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [4096, 14336]\n",
            "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [1024, 4096]\n",
            "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [1024, 4096]\n",
            "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [14336, 4096]\n",
            "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [14336, 4096]\n",
            "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [4096, 14336]\n",
            "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [1024, 4096]\n",
            "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [1024, 4096]\n",
            "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [14336, 4096]\n",
            "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [14336, 4096]\n",
            "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [4096, 14336]\n",
            "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [1024, 4096]\n",
            "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [1024, 4096]\n",
            "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [14336, 4096]\n",
            "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [14336, 4096]\n",
            "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [4096, 14336]\n",
            "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [1024, 4096]\n",
            "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [1024, 4096]\n",
            "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [14336, 4096]\n",
            "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [14336, 4096]\n",
            "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [4096, 14336]\n",
            "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [1024, 4096]\n",
            "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [1024, 4096]\n",
            "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [14336, 4096]\n",
            "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [14336, 4096]\n",
            "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [4096, 14336]\n",
            "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [1024, 4096]\n",
            "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [1024, 4096]\n",
            "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [14336, 4096]\n",
            "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [14336, 4096]\n",
            "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [4096, 14336]\n",
            "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [1024, 4096]\n",
            "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [1024, 4096]\n",
            "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [14336, 4096]\n",
            "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [14336, 4096]\n",
            "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [4096, 14336]\n",
            "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]\n",
            "model.norm.weight                                -> output_norm.weight                       | F16    | [4096]\n",
            "lm_head.weight                                   -> output.weight                            | F16    | [128256, 4096]\n",
            "Writing model_gsm8k-unsloth.F16.gguf, format 1\n",
            "Ignoring added_tokens.json since model matches vocab size without it.\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "gguf: Adding 280147 merge(s).\n",
            "gguf: Setting special token type bos to 128000\n",
            "gguf: Setting special token type eos to 128001\n",
            "gguf: Setting special token type pad to 128001\n",
            "[  1/291] Writing tensor token_embd.weight                      | size 128256 x   4096  | type F16  | T+   8\n",
            "[  2/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  28\n",
            "[  3/291] Writing tensor blk.0.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  28\n",
            "[  4/291] Writing tensor blk.0.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  28\n",
            "[  5/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+  28\n",
            "[  6/291] Writing tensor blk.0.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  29\n",
            "[  7/291] Writing tensor blk.0.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  31\n",
            "[  8/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  32\n",
            "[  9/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+  32\n",
            "[ 10/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+  32\n",
            "[ 11/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  32\n",
            "[ 12/291] Writing tensor blk.1.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  32\n",
            "[ 13/291] Writing tensor blk.1.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  32\n",
            "[ 14/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+  33\n",
            "[ 15/291] Writing tensor blk.1.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  34\n",
            "[ 16/291] Writing tensor blk.1.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  35\n",
            "[ 17/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  37\n",
            "[ 18/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+  37\n",
            "[ 19/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+  37\n",
            "[ 20/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  37\n",
            "[ 21/291] Writing tensor blk.2.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  37\n",
            "[ 22/291] Writing tensor blk.2.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  37\n",
            "[ 23/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+  37\n",
            "[ 24/291] Writing tensor blk.2.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  39\n",
            "[ 25/291] Writing tensor blk.2.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  40\n",
            "[ 26/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  40\n",
            "[ 27/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+  42\n",
            "[ 28/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+  42\n",
            "[ 29/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  42\n",
            "[ 30/291] Writing tensor blk.3.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  42\n",
            "[ 31/291] Writing tensor blk.3.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  42\n",
            "[ 32/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+  42\n",
            "[ 33/291] Writing tensor blk.3.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  43\n",
            "[ 34/291] Writing tensor blk.3.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  44\n",
            "[ 35/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  47\n",
            "[ 36/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+  51\n",
            "[ 37/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+  51\n",
            "[ 38/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  51\n",
            "[ 39/291] Writing tensor blk.4.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  51\n",
            "[ 40/291] Writing tensor blk.4.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  51\n",
            "[ 41/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+  52\n",
            "[ 42/291] Writing tensor blk.4.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  53\n",
            "[ 43/291] Writing tensor blk.4.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  53\n",
            "[ 44/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  56\n",
            "[ 45/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  56\n",
            "[ 46/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  56\n",
            "[ 47/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  56\n",
            "[ 48/291] Writing tensor blk.5.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  56\n",
            "[ 49/291] Writing tensor blk.5.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  56\n",
            "[ 50/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+  57\n",
            "[ 51/291] Writing tensor blk.5.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  57\n",
            "[ 52/291] Writing tensor blk.5.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  59\n",
            "[ 53/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  59\n",
            "[ 54/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  62\n",
            "[ 55/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  62\n",
            "[ 56/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  62\n",
            "[ 57/291] Writing tensor blk.6.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  62\n",
            "[ 58/291] Writing tensor blk.6.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  62\n",
            "[ 59/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+  62\n",
            "[ 60/291] Writing tensor blk.6.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  63\n",
            "[ 61/291] Writing tensor blk.6.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  64\n",
            "[ 62/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  67\n",
            "[ 63/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  67\n",
            "[ 64/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  67\n",
            "[ 65/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  67\n",
            "[ 66/291] Writing tensor blk.7.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  67\n",
            "[ 67/291] Writing tensor blk.7.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  67\n",
            "[ 68/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+  67\n",
            "[ 69/291] Writing tensor blk.7.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  69\n",
            "[ 70/291] Writing tensor blk.7.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  70\n",
            "[ 71/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  72\n",
            "[ 72/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  76\n",
            "[ 73/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  76\n",
            "[ 74/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  76\n",
            "[ 75/291] Writing tensor blk.8.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  77\n",
            "[ 76/291] Writing tensor blk.8.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  77\n",
            "[ 77/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+  77\n",
            "[ 78/291] Writing tensor blk.8.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  78\n",
            "[ 79/291] Writing tensor blk.8.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  79\n",
            "[ 80/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  79\n",
            "[ 81/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  82\n",
            "[ 82/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  82\n",
            "[ 83/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  82\n",
            "[ 84/291] Writing tensor blk.9.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  82\n",
            "[ 85/291] Writing tensor blk.9.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  84\n",
            "[ 86/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+  84\n",
            "[ 87/291] Writing tensor blk.9.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  84\n",
            "[ 88/291] Writing tensor blk.9.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  84\n",
            "[ 89/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  85\n",
            "[ 90/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  85\n",
            "[ 91/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  85\n",
            "[ 92/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  85\n",
            "[ 93/291] Writing tensor blk.10.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  85\n",
            "[ 94/291] Writing tensor blk.10.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  86\n",
            "[ 95/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+  87\n",
            "[ 96/291] Writing tensor blk.10.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  87\n",
            "[ 97/291] Writing tensor blk.10.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  88\n",
            "[ 98/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  89\n",
            "[ 99/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  89\n",
            "[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  89\n",
            "[101/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  89\n",
            "[102/291] Writing tensor blk.11.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  89\n",
            "[103/291] Writing tensor blk.11.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  89\n",
            "[104/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+  92\n",
            "[105/291] Writing tensor blk.11.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  92\n",
            "[106/291] Writing tensor blk.11.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  92\n",
            "[107/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  93\n",
            "[108/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  93\n",
            "[109/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  93\n",
            "[110/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  93\n",
            "[111/291] Writing tensor blk.12.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  93\n",
            "[112/291] Writing tensor blk.12.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  93\n",
            "[113/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+  94\n",
            "[114/291] Writing tensor blk.12.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  95\n",
            "[115/291] Writing tensor blk.12.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  96\n",
            "[116/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  97\n",
            "[117/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  98\n",
            "[118/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  98\n",
            "[119/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  98\n",
            "[120/291] Writing tensor blk.13.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  98\n",
            "[121/291] Writing tensor blk.13.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  98\n",
            "[122/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+  98\n",
            "[123/291] Writing tensor blk.13.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  99\n",
            "[124/291] Writing tensor blk.13.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 101\n",
            "[125/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 102\n",
            "[126/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+ 102\n",
            "[127/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+ 102\n",
            "[128/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 102\n",
            "[129/291] Writing tensor blk.14.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 107\n",
            "[130/291] Writing tensor blk.14.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 107\n",
            "[131/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 107\n",
            "[132/291] Writing tensor blk.14.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 107\n",
            "[133/291] Writing tensor blk.14.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 108\n",
            "[134/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 112\n",
            "[135/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+ 112\n",
            "[136/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+ 112\n",
            "[137/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 112\n",
            "[138/291] Writing tensor blk.15.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 113\n",
            "[139/291] Writing tensor blk.15.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 113\n",
            "[140/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 113\n",
            "[141/291] Writing tensor blk.15.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 115\n",
            "[142/291] Writing tensor blk.15.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 115\n",
            "[143/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 116\n",
            "[144/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+ 117\n",
            "[145/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+ 117\n",
            "[146/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 117\n",
            "[147/291] Writing tensor blk.16.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 117\n",
            "[148/291] Writing tensor blk.16.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 117\n",
            "[149/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 117\n",
            "[150/291] Writing tensor blk.16.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 118\n",
            "[151/291] Writing tensor blk.16.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 119\n",
            "[152/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 120\n",
            "[153/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+ 122\n",
            "[154/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+ 122\n",
            "[155/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 122\n",
            "[156/291] Writing tensor blk.17.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 122\n",
            "[157/291] Writing tensor blk.17.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 122\n",
            "[158/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 123\n",
            "[159/291] Writing tensor blk.17.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 126\n",
            "[160/291] Writing tensor blk.17.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 127\n",
            "[161/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 128\n",
            "[162/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+ 128\n",
            "[163/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+ 128\n",
            "[164/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 128\n",
            "[165/291] Writing tensor blk.18.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 128\n",
            "[166/291] Writing tensor blk.18.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 128\n",
            "[167/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 128\n",
            "[168/291] Writing tensor blk.18.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 130\n",
            "[169/291] Writing tensor blk.18.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 130\n",
            "[170/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 132\n",
            "[171/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+ 133\n",
            "[172/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+ 133\n",
            "[173/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 133\n",
            "[174/291] Writing tensor blk.19.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 133\n",
            "[175/291] Writing tensor blk.19.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 133\n",
            "[176/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 133\n",
            "[177/291] Writing tensor blk.19.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 135\n",
            "[178/291] Writing tensor blk.19.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 137\n",
            "[179/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 137\n",
            "[180/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+ 142\n",
            "[181/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+ 142\n",
            "[182/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 142\n",
            "[183/291] Writing tensor blk.20.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 142\n",
            "[184/291] Writing tensor blk.20.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 142\n",
            "[185/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 142\n",
            "[186/291] Writing tensor blk.20.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 145\n",
            "[187/291] Writing tensor blk.20.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 145\n",
            "[188/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 146\n",
            "[189/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+ 146\n",
            "[190/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+ 146\n",
            "[191/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 146\n",
            "[192/291] Writing tensor blk.21.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 147\n",
            "[193/291] Writing tensor blk.21.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 147\n",
            "[194/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 148\n",
            "[195/291] Writing tensor blk.21.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 148\n",
            "[196/291] Writing tensor blk.21.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 149\n",
            "[197/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 150\n",
            "[198/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+ 152\n",
            "[199/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+ 152\n",
            "[200/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 152\n",
            "[201/291] Writing tensor blk.22.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 152\n",
            "[202/291] Writing tensor blk.22.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 152\n",
            "[203/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 152\n",
            "[204/291] Writing tensor blk.22.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 154\n",
            "[205/291] Writing tensor blk.22.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 155\n",
            "[206/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 156\n",
            "[207/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+ 157\n",
            "[208/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+ 157\n",
            "[209/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 157\n",
            "[210/291] Writing tensor blk.23.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 157\n",
            "[211/291] Writing tensor blk.23.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 157\n",
            "[212/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 158\n",
            "[213/291] Writing tensor blk.23.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 159\n",
            "[214/291] Writing tensor blk.23.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 162\n",
            "[215/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 162\n",
            "[216/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+ 163\n",
            "[217/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+ 163\n",
            "[218/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 163\n",
            "[219/291] Writing tensor blk.24.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 163\n",
            "[220/291] Writing tensor blk.24.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 163\n",
            "[221/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 163\n",
            "[222/291] Writing tensor blk.24.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 164\n",
            "[223/291] Writing tensor blk.24.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 165\n",
            "[224/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 167\n",
            "[225/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+ 172\n",
            "[226/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+ 172\n",
            "[227/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 172\n",
            "[228/291] Writing tensor blk.25.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 172\n",
            "[229/291] Writing tensor blk.25.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 172\n",
            "[230/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 172\n",
            "[231/291] Writing tensor blk.25.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 174\n",
            "[232/291] Writing tensor blk.25.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 175\n",
            "[233/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 175\n",
            "[234/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+ 176\n",
            "[235/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+ 176\n",
            "[236/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 176\n",
            "[237/291] Writing tensor blk.26.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 176\n",
            "[238/291] Writing tensor blk.26.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 176\n",
            "[239/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 176\n",
            "[240/291] Writing tensor blk.26.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 177\n",
            "[241/291] Writing tensor blk.26.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 178\n",
            "[242/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 179\n",
            "[243/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+ 183\n",
            "[244/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+ 183\n",
            "[245/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 183\n",
            "[246/291] Writing tensor blk.27.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 183\n",
            "[247/291] Writing tensor blk.27.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 183\n",
            "[248/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 183\n",
            "[249/291] Writing tensor blk.27.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 184\n",
            "[250/291] Writing tensor blk.27.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 185\n",
            "[251/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 186\n",
            "[252/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+ 188\n",
            "[253/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+ 188\n",
            "[254/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 188\n",
            "[255/291] Writing tensor blk.28.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 188\n",
            "[256/291] Writing tensor blk.28.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 188\n",
            "[257/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 188\n",
            "[258/291] Writing tensor blk.28.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 191\n",
            "[259/291] Writing tensor blk.28.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 191\n",
            "[260/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 191\n",
            "[261/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+ 192\n",
            "[262/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+ 192\n",
            "[263/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 192\n",
            "[264/291] Writing tensor blk.29.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 192\n",
            "[265/291] Writing tensor blk.29.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 192\n",
            "[266/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 192\n",
            "[267/291] Writing tensor blk.29.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 193\n",
            "[268/291] Writing tensor blk.29.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 195\n",
            "[269/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 195\n",
            "[270/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+ 199\n",
            "[271/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+ 199\n",
            "[272/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 199\n",
            "[273/291] Writing tensor blk.30.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 199\n",
            "[274/291] Writing tensor blk.30.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 199\n",
            "[275/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 199\n",
            "[276/291] Writing tensor blk.30.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 201\n",
            "[277/291] Writing tensor blk.30.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 204\n",
            "[278/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 204\n",
            "[279/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+ 206\n",
            "[280/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+ 206\n",
            "[281/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 206\n",
            "[282/291] Writing tensor blk.31.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 206\n",
            "[283/291] Writing tensor blk.31.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 206\n",
            "[284/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 206\n",
            "[285/291] Writing tensor blk.31.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 208\n",
            "[286/291] Writing tensor blk.31.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 209\n",
            "[287/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 214\n",
            "[288/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+ 214\n",
            "[289/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+ 214\n",
            "[290/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+ 214\n",
            "[291/291] Writing tensor output.weight                          | size 128256 x   4096  | type F16  | T+ 223\n",
            "Wrote model_gsm8k-unsloth.F16.gguf\n",
            "Unsloth: Conversion completed! Output location: ./model_gsm8k-unsloth.F16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\n",
            "main: build = 2776 (c4ec9c0d)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing './model_gsm8k-unsloth.F16.gguf' to './model_gsm8k-unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n",
            "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./model_gsm8k-unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = .\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ä  Ä \", \"Ä  Ä Ä Ä \", \"Ä Ä  Ä Ä \", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "[   1/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q4_K .. size =  1002.00 MiB ->   281.81 MiB\n",
            "[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[   7/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[   8/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  16/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  17/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  25/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  26/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  34/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  35/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  43/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  44/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  52/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  53/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  61/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  62/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  70/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  71/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  79/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  80/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  88/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  89/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  97/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  98/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 106/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 107/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 115/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 116/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 124/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 125/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 133/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 134/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 142/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 143/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 151/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 152/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 160/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 161/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 169/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 170/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 178/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 179/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 187/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 188/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 196/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 197/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 205/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 206/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 214/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 215/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 223/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 224/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 232/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 233/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 241/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 242/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 250/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 251/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 259/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 260/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 268/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 269/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 277/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 278/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 286/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 287/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 291/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB\n",
            "llama_model_quantize_internal: model size  = 15317.02 MB\n",
            "llama_model_quantize_internal: quant size  =  4685.30 MB\n",
            "Unsloth: Conversion completed! Output location: ./model_gsm8k-unsloth.Q4_K_M.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AYbHiz2Zhsfk",
        "outputId": "644b553b-d846-45b0-b0f9-88be53da68c1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZdnpPHwaSB0",
        "outputId": "8338f336-d1f2-4450-b0ca-a7f4ea12dd80"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nm83Y8jVhLH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy model file to google drive\n",
        "import shutil\n",
        "import os\n",
        "source_file = '/content/model_gsm8k-unsloth.Q4_K_M.gguf'\n",
        "destination_dir = '/content/drive/MyDrive/Colab Notebooks/unsloth/Llama3'\n",
        "destination_file = os.path.join(destination_dir, 'model_gsm8k-unsloth.Q4_K_M.gguf')\n",
        "shutil.copy(source_file, destination_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fXlB4F9Zab3v",
        "outputId": "9ff96e6e-c63a-4a81-b9e8-6e85242ab489"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/unsloth/Llama3/model_gsm8k-unsloth.Q4_K_M.gguf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_file = '/content/model_gsm8k-unsloth.F16.gguf'\n",
        "destination_dir = '/content/drive/MyDrive/Colab Notebooks/unsloth/Llama3'\n",
        "destination_file = os.path.join(destination_dir, 'model_gsm8k-unsloth.F16.gguf')\n",
        "shutil.copy(source_file, destination_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_WV0TLCz6hTK",
        "outputId": "0c491a52-46f8-4e4a-980f-0a8f66a9a6aa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/unsloth/Llama3/model_gsm8k-unsloth.F16.gguf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}